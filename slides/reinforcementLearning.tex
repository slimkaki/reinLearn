\documentclass{beamer}
\usepackage[utf8]{inputenc}

\usetheme{Madrid}
\usecolortheme{default}
\usepackage{color}
\usepackage{colortbl}

\usepackage{algorithmic}

\begin{document}

\title{Aprendizagem por Reforço} 
\author{Fabrício Barth}
\institute{Insper Instituto de Ensino e Pesquisa}
\date{Outubro de 2022}

\maketitle

\begin{frame}{Contexto}
Até o momento vimos nesta disciplina:
\begin{itemize}
	\item Conceito de Agente Autônomo;
	\item Solução de problemas usando busca em espaço de estados:
	\begin{itemize}
		\item Algoritmos de busca cega, e;
		\item Algoritmos de busca informados.
	\end{itemize}
	\item Busca competitiva.
\end{itemize}
\end{frame}

\begin{frame}{Conteúdo desta aula e das próximas}
  \begin{itemize}
\item Visão Geral sobre Aprendizagem por Reforço
\item Algoritmo Q-Learning
\item Implementações com o projeto \textbf{OpenAI Gym}
%\item Considerações Finais
%\item Material de Consulta
  \end{itemize}
\end{frame}

\begin{frame}{Ao final deste material você saberá}

	\begin{itemize}
	\item o que é \textbf{Aprendizagem por Reforço} e os seus principais conceitos;
	\item como desenvolver um agente autônomo usando o algoritmo \textbf{Q-Learning}, e;
	\item quais os aspectos positivos e negativos do algoritmo \textbf{Q-Learning}.
	%\item como o algoritmo \textbf{Q-Learning} funciona e como implementá-lo;
	%\item como implementar um \textbf{agente autônomo} usando aprendizagem por reforço, e;
	%\item como implementar um agente autônomo para atuar nos ambientes do \textbf{projeto \textsc{Gym}}.
	\end{itemize}

\end{frame}

\begin{frame}{Taxi Driver - OpenAI Gym}

	\begin{center}
		\includegraphics[width=.8\textwidth]{figuras/taxi_driver.png}
	\end{center}

	Podemos implementar uma solução para este problema usando o algoritmo $A^{*}$. Neste caso, 
	\textbf{o que é necessário fazer?}

\end{frame}

\begin{frame}{Taxi Driver - OpenAI Gym}

	Definir uma \textbf{Heurística $H$} que seja admissível e que traga algum valor para o 
	processo de busca.

\end{frame}

\begin{frame}{Ambientes competitivos}

	\begin{center}
		\includegraphics[width=.6\textwidth]{figuras/chess.jpg}
	\end{center}

	Podemos implementar uma solução para este tipo de problema usando o algoritmo 
	\textsc{Min-Max}. Neste caso, \emph{o que é necessário fazer?}

\end{frame}

\begin{frame}{Ambientes competitivos}

	Definir uma \textbf{função de utilidade} que consegue descrever a utilidade dos estados possíveis para o meu agente.


\end{frame}

\begin{frame}{Questão}

	E se fosse possível desenvolver um agente autônomo sem ter que codificar nenhum
	conhecimento sobre a tarefa que ele precisa executar (heurísticas ou funções de utilidade específicas)?

\end{frame}


\begin{frame}{Aprendizagem por Reforço: Visão Geral}
	\small
	Um agente aprende a resolver uma tarefa através de repetidas interações com o ambiente, por tentativa e erro, recebendo 
	(esporadicamente) reforços (punições ou recompensas) como retorno.
  \begin{center}
	\includegraphics[width=.7\textwidth]{figuras/visaoGeral.png}
\end{center}

\end{frame}

\begin{frame}{Aprendizagem por Reforço: Visão Geral}
	\begin{itemize}
	\item<1-> Este agente não tem conhecimento algum sobre a tarefa que precisa executar (heurísticas ou funções de utilidade específicas).
	\item<2-> Sendo assim, \textbf{{\color{red}como é que o agente consegue atingir um determinado objetivo?}}
	\item<3-> O agente aprende uma \textbf{política de controle}, executando uma \textbf{sequência de ações} e observando as suas \textbf{consequências}.
	
	\end{itemize}
\end{frame}
	



\begin{frame}{Política de Controle}
	\begin{itemize}
	\item A política de controle desejada é aquela que \textbf{maximiza} os reforços 
	(\textit{reward}) acumulados ao longo do tempo pelo 
	agente: $r_{0} + \gamma r_{1} + \gamma^{2} r_{2} + \cdots $ onde $0 \leq \gamma < 1$.
	\end{itemize}

%\hspace{1cm}

  \begin{center}
	\includegraphics[width=.7\textwidth]{figuras/sequencia_futuro.png}
\end{center}

\pause

\begin{itemize}
	\item O $V(s_{1})$ será a soma de $r_{1}$ com o $V(s_{2})$. No entanto, considerando 
	o fator de desconto $\gamma$, temos: $V(s_{1}) = r_{1} + \gamma V(s_{2}) $.
	
	\item O valor de um estado final leva-se em consideração apenas o 
	reforço: $V(s_{n}) = r_{n}$.
	
	%\item Resumindo, temos: $V(s) = \sum_{t} \gamma^{t} R$
\end{itemize}
\end{frame}


\begin{frame}{Exemplo}
	\begin{center}
		\begin{tabular}{ |c|c|c|c| } 
			\hline
			\cellcolor{red!25} Início & Campo & Campo & Campo \\ 
			\hline
			Campo & \cellcolor{black!25} Buraco & Campo &  \cellcolor{black!25} Buraco\\ 
			\hline
			Campo & Campo & Campo & \cellcolor{black!25} Buraco\\ 
			\hline
			\cellcolor{black!25} Buraco & Campo & Campo & \textbf{Objetivo} \\ 
			\hline
		\end{tabular}
	\end{center}
	
	\vspace{0.2cm}
	\small
	\textbf{Ações que o agente sabe executar}: 
	\begin{enumerate}
		\item Mover para Baixo ($\downarrow$)
		\item Mover para Cima ($\uparrow$)
		\item Mover para Direita ($\rightarrow$)
		\item Mover para Esquerda ($\leftarrow$)
	\end{enumerate}
	
\end{frame}


\begin{frame}{Exemplo}
	\begin{itemize}
		\item Considerando que o local do objetivo, dos buracos e dos campos serão sempre 
		os mesmos então temos \textbf{16} estados possíveis. 
		\item Este problema tem \textbf{4} ações possíveis. 
		\item Se o agente cair em um buraco ele recebe \textbf{-1} como recompensa, se ele ir para um 
		campo ele recebe \textbf{0} e ao chegar no objetivo ele recebe \textbf{1}.  
		%\item Para que agente possa identificar uma política de controle ótima este agente 
		%precisa criar um \textbf{mapeamento} entre \textbf{estados} (S) e \textbf{ações} (A)
		
	\end{itemize}
\end{frame}


\begin{frame}{Considere o seguinte estado}
	
	\begin{center}
		\begin{tabular}{ |c|c|c|c| } 
			\hline
			Início & Campo & Campo & Campo \\ 
			\hline
			Campo & \cellcolor{black!25} Buraco & Campo &  \cellcolor{black!25} Buraco\\ 
			\hline
			Campo & Campo & Campo & \cellcolor{black!25} Buraco\\ 
			\hline
			\cellcolor{black!25} Buraco & Campo & \cellcolor{red!25} Campo & \textbf{Objetivo} \\ 
			\hline
		\end{tabular}
	\end{center}
	
	\vspace{0.2cm}
	
	\begin{itemize}
		\item Se o agente for para $\rightarrow$ (direita) então $V(s_{n}) = 1$
	\end{itemize}
		
\end{frame}


\begin{frame}{Considere um estado anterior $s_{n-1}$}
	
	\begin{center}
		\begin{tabular}{ |c|c|c|c| } 
			\hline
			Início & Campo & Campo & Campo \\ 
			\hline
			Campo & \cellcolor{black!25} Buraco & Campo &  \cellcolor{black!25} Buraco\\ 
			\hline
			Campo & Campo & \cellcolor{red!25} Campo & \cellcolor{black!25} Buraco\\ 
			\hline
			\cellcolor{black!25} Buraco & Campo & Campo & \textbf{Objetivo} \\ 
			\hline
		\end{tabular}
	\end{center}
	
	\vspace{0.2cm}
	
	\begin{itemize}
		\item Se o agente for para $\rightarrow$ (direita) então $V(s_{n-1}) = -1$
		\item Se o agente for para $\downarrow$ (baixo) então:
		\begin{eqnarray}
		V(s_{n-1}) = r(s_{n-1}) + \gamma V(s_{n}) \\ \nonumber
		V(s_{n-1}) = 0 + \gamma \times 1 \nonumber
		\end{eqnarray}
	\end{itemize}
	
\end{frame}

\begin{frame}{Considere um estado anterior $s_{n-2}$}
	
	\begin{center}
		\begin{tabular}{ |c|c|c|c| } 
			\hline
			Início & Campo & Campo & Campo \\ 
			\hline
			Campo & \cellcolor{black!25} Buraco & \cellcolor{red!25} Campo &  \cellcolor{black!25} Buraco\\ 
			\hline
			Campo & Campo & Campo & \cellcolor{black!25} Buraco\\ 
			\hline
			\cellcolor{black!25} Buraco & Campo & Campo & \textbf{Objetivo} \\ 
			\hline
		\end{tabular}
	\end{center}
	
	\vspace{0.2cm}
	
	\begin{itemize}
		\item Se o agente for para $\rightarrow$ (direita) então $V(s_{n-2}) = -1$
		\item Se o agente for para $\leftarrow$ (esquerda) então $V(s_{n-2}) = -1$
		\item Se o agente for para $\downarrow$ (baixo) então:
		\begin{eqnarray}
			V(s_{n-2}) = r(s_{n-2}) + \gamma V(s_{n-1}) + \gamma^{2} V(s_{n}) \\ \nonumber
			V(s_{n-1}) = 0 + \gamma \times (\gamma^{2} \times 1) + \gamma^{2} \times 1 \nonumber
		\end{eqnarray}
	\end{itemize}
	
\end{frame}


\begin{frame}{Fator de desconto $\gamma$}
	
	\begin{itemize}
		%\item Para tarefas episódicas, o retorno é fácil de ser calculado, 
		%pois será a soma de todas as recompensas obtidas pelo agente. 
		%Mas para tarefas contínuas, como a atividade não tem fim e não podemos 
		%somar até o infinito, há a necessidade da inserção de um fator de 
		%desconto ($\gamma$).
		\item O fator de desconto ($\gamma$) é um hiperparâmetro que consiste 
		em um número entre 0 e 1 que define a importância das recompensas futuras 
		em relação a atual	($0 \leq \gamma < 1$).
		\item Valores mais próximos ao 0 dão mais importância a recompensas 
		imediatas enquanto os mais próximos de 1 tentarão manter a importância 
		de recompensas futuras.
	\end{itemize}
\end{frame}




\begin{frame}{Algoritmo Q-Learning}
	
Para que agente possa identificar uma política de controle ótima este agente 
precisa criar um \textbf{mapeamento} entre \textbf{estados} ($S$) e \textbf{ações} ($A$).

\end{frame}

\begin{frame}{Algoritmo Q-Learning}
	
\begin{itemize}
	
	\item Este mapeamento pode ser representado por uma função $Q(S,A)$ onde $S$ são 
	todos os estados possíveis ($s_{1}, s_{2}, \cdots$) e onde $A$ são todas as 
	ações possíveis ($a_{1}, a_{2}, \cdots$)

\end{itemize}

	\begin{center}
	\begin{tabular}{ |c|c|c|c|c| } 
		\hline
		 \textbf{Q-table}  & $a_{1}$ & $a_{2}$ & $a_{3}$& $a_{4}$ \\
		 \hline
		$s_{1}$&  &  &  & \\ 
		\hline
		$s_{2}$&  &  &  & \\ 
\hline
		$\cdots$&  &  &  & \\ 
\hline
		$s_{n}$&  &  &  & \\ 
\hline
	\end{tabular}
\end{center}

\begin{itemize}
	\item Para criar um \textbf{mapeamento} $Q(S,A)$ é necessário executar o 
	agente no ambiente considerando o \textbf{reforço} dado por cada ação. 
\end{itemize}

\end{frame}


\begin{frame}{Algoritmo Q-Learning}

  \begin{center}
	\includegraphics[width=\textwidth]{figuras/mapa1.png}
\end{center}

\end{frame}

\begin{frame}{Algoritmo Q-Learning}

	\begin{block}{}
		Como é que o agente pode saber quais são as melhores ações em cada estado?
	\end{block}

\pause

	\begin{itemize}
		\item A ideia é fazer com que o agente aprenda a função de mapeamento $Q(S,A)$. 
		Ou seja, que seja capaz de identificar qual é a melhor ação para cada estado 
		através das suas \textbf{experiências}. 
		\item \textit{Testando} \textbf{infinitas} vezes o ambiente. 
		Ou seja, \textit{testando} \textbf{muitas} vezes as combinações entre 
		\textbf{estados} ($S$) e \textbf{ações} ($A$). 
	\end{itemize}

\end{frame}

\begin{frame}{Algoritmo Q-Learning}
\begin{small}
\begin{center}
	\begin{tabular}{ |c|c|c|c| } 
		\hline
		\cellcolor{red!25} Início & Campo & Campo & Campo \\ 
		\hline
		Campo & \cellcolor{black!25} Buraco & Campo &  \cellcolor{black!25} Buraco\\ 
		\hline
		 Campo & Campo & Campo & \cellcolor{black!25} Buraco\\ 
		\hline
		\cellcolor{black!25} Buraco & Campo & Campo & \textbf{Objetivo} \\ 
		\hline
	\end{tabular}
\end{center}
\end{small}

Primeiro episódio ($\gamma = 0.9$):
\begin{eqnarray}
Q(s_{1}, baixo) \leftarrow  r + \gamma \max_{a'}{Q(s', a')} \nonumber \\
Q(s_{1}, baixo) \leftarrow  0 + 0.9 \times \max[0, 0, 0] \nonumber \\ \nonumber \\
Q(s_{2}, direita) \leftarrow  -1 + 0.9 \times \max[0, 0, 0,0] \nonumber 
%Q(s_{n}, direita) \leftarrow  1 + 0.9 \times \max[0, 0, 0,0] \nonumber 
\end{eqnarray}

\end{frame}

\begin{frame}{Algoritmo Q-Learning}

Q-table resultante da execução do $1^{o}$ episódio. 

	\begin{center}
	\begin{tabular}{ |c|c|c|c|c| } 
		\hline
		\textbf{Q-table}  & $esquerda$ & $baixo$ & $direita$& $cima$ \\
		\hline
		$s_{1}$& 0  & 0 & 0 & 0\\ 
		\hline
		$s_{2}$& 0 & 0  & -1 & 0\\ 
		\hline
$s_{3}$& 0 & 0 & 0 & 0\\ 
\hline
$s_{4}$& 0 & 0  & 0  &  0\\ 
\hline
		$\cdots$& $\cdots$ & $\cdots$ & $\cdots$ &$\cdots$ \\ 
		\hline
		$s_{n}$& 0 & 0  & 0  & 0 \\ 
		\hline
	\end{tabular}
\end{center}
\end{frame}

%\begin{frame}{Algoritmo Q-Learning}
%
%Já na execução do $2^{o}$ episódio... 
%
%	\begin{center}
%	\begin{tabular}{ |c|c|c|c| } 
%		\hline
%	 Início & Campo & Campo & Campo \\ 
%		\hline
%		Campo & \cellcolor{black!25} Buraco & Campo (0.0) &  \cellcolor{black!25} Buraco\\ 
%		\hline
%		Campo & Campo (0.0) & \cellcolor{red!25}Campo & \cellcolor{black!25} Buraco (-0.1)\\ 
%		\hline
%		\cellcolor{black!25} Buraco & Campo & Campo (0.9) & \textbf{Objetivo} \\ 
%		\hline
%	\end{tabular}
%\end{center}
%
%\end{frame}

\begin{frame}{Algoritmo Q-Learning}

Q-table resultante da execução do $n$-éssimo episódio. 

\begin{center}
	\begin{tabular}{ |c|c|c|c|c| } 
		\hline
		\textbf{Q-table}  & $esquerda$ & $baixo$ & $direita$& $cima$ \\
		\hline
		$s_{1}$& 0.02  & 0.03 & 0.0001 & 0.0001\\ 
		\hline
		$s_{2}$& 0.00 & 0.05 & -0.003 & 0.001\\ 
		\hline
		$\cdots$& $\cdots$ & $\cdots$ & $\cdots$ &$\cdots$ \\ 
		\hline
		$s_{n}$& 0.985 & 0.0001  & 0.003  & 0.002 \\ 
		\hline
	\end{tabular}
\end{center}

Após a execução de $n$ episódios o agente conhece qual a melhor ação para cada estado. 

\end{frame}


\begin{frame}{Algoritmo Q-Learning} 
	\begin{algorithmic} 
		\STATE \emph{\textbf{function} Q-Learning(env, $\alpha$, $\gamma$, episódios)}
		\STATE inicializar os valores de $Q(s, a)$ arbitrariamente
		\FOR {todos os episódios}
		\STATE inicializar $s$ a partir de $env$
		\REPEAT
		\STATE escolher uma ação $a$ para um estado $s$
		\STATE executar a ação $a$
		\STATE observar a recompensa $r$ e o novo estado $s'$ 
		\STATE $Q(s,a) \leftarrow Q(s,a) + \alpha [r +\gamma \max_{a'}{Q(s', a')} - Q(s,a)]$
		\STATE$s  \leftarrow s'$
		\UNTIL {$s$ ser um estado final}
		\ENDFOR
		\STATE \textbf{return} $Q(s, a)$
	\end{algorithmic}
\end{frame}


\begin{frame}{Atividade de implementação}
	
	\begin{block}{Setup do ambiente}
		Faça o clone do projeto https://github.com/fbarth/reinLearn
		\href{https://github.com/fbarth/reinLearn}{\beamergotobutton{Link}}
	\end{block}
	
	\begin{alertblock}{Atualização da Q-table}
		O objetivo desta atividade é implementar a rotira responsável pela atualização da \textit{Q-table} no arquivo QLearning.py
	\end{alertblock}
	
	\begin{block}{Atividades}
		Siga o roteiro descrito em src/parte1/parte1.md \href{https://github.com/fbarth/reinLearn/blob/main/src/parte1/parte1.md}
		{\beamergotobutton{Link}}
	\end{block}

\end{frame}

%%\begin{frame}{Algoritmo Q-Learning} 
%%\begin{algorithmic} 
%%	\STATE \textbf{function} Q-Learning(env, $\alpha$, $\gamma$, episódios)
%%	\STATE \emph{inicializar os valores de $Q(s, a)$ arbitrariamente}
%%	\FOR {todos os episódios}
%%	\STATE inicializar $s$ a partir de $env$
%%	\REPEAT
%%	\STATE escolher uma ação $a$ para um estado $s$
%%	\STATE executar a ação $a$
%%	\STATE observar a recompensa $r$ e o novo estado $s'$ 
%%	\STATE $Q(s,a) \leftarrow Q(s,a) + \alpha [r +\gamma \max_{a'}{Q(s', a')} - Q(s,a)]$
%%	\STATE$s  \leftarrow s'$
%%	\UNTIL {$s$ ser um estado final}
%%	\ENDFOR
%%	\STATE \textbf{return} $Q(s, a)$
%%\end{algorithmic}
%
%\end{frame}
%

%\begin{frame}{Algoritmo Q-Learning} 
%\begin{algorithmic} 
%	\STATE \textbf{function} Q-Learning(env, $\alpha$, $\gamma$, episódios)
%	\STATE inicializar os valores de $Q(s, a)$ arbitrariamente
%	\FOR {todos os episódios}
%	\STATE inicializar $s$ a partir de $env$
%	\REPEAT
%	\STATE escolher uma ação $a$ para um estado $s$
%	\STATE executar a ação $a$
%	\STATE observar a recompensa $r$ e o novo estado $s'$ 
%	\STATE \emph{$Q(s,a) \leftarrow Q(s,a) + \alpha [r +\gamma \max_{a'}{Q(s', a')} - Q(s,a)]$}
%	\STATE$s  \leftarrow s'$
%	\UNTIL {$s$ ser um estado final}
%	\ENDFOR
%	\STATE \textbf{return} $Q(s, a)$
%\end{algorithmic}
%
%\end{frame}
%

\begin{frame}{Algoritmo Q-Learning: hiperparâmetro $\alpha$}
	\begin{itemize}
		\item $\alpha$ é a taxa de aprendizado ($0 < \alpha \leq 1$), quanto maior, mais valor dá ao novo aprendizado.
	\end{itemize}
\end{frame}

\begin{frame}{Que ação escolher?} 
	\small
	\begin{algorithmic} 
		\STATE \textbf{function} Q-Learning(env, $\alpha$, $\gamma$, episódios)
		\STATE inicializar os valores de $Q(s, a)$ arbitrariamente
		\FOR {todos os episódios}
		\STATE inicializar $s$ a partir de $env$
		\REPEAT
		\STATE \emph{escolher uma ação $a$ para um estado $s$}
		\STATE executar a ação $a$
		\STATE observar a recompensa $r$ e o novo estado $s'$ 
		\STATE $Q(s,a) \leftarrow Q(s,a) + \alpha [r +\gamma \max_{a'}{Q(s', a')} - Q(s,a)]$
		\STATE$s  \leftarrow s'$
		\UNTIL {$s$ ser um estado final}
		\ENDFOR
		\STATE \textbf{return} $Q(s, a)$
	\end{algorithmic}
\end{frame}


\begin{frame}{\textit{Exploration} vs \textit{Exploitation}}
	\begin{itemize}
		\item<1-> A política que o agente utiliza para escolher 
		uma ação $a$ para um estado $s$ não interfere no aprendizado 
		da \textit{Q-table}.
		\item<2-> No entanto, para que o algoritmo \textit{Q-learning} possa convergir 
		para um determinado problema é necessário que o algoritmo visite pares de 
		ação-estado infinitas (muitas) vezes.
		\item<3-> Por isso, que a escolha de determinada \textit{ação} em um \textit{estado} 
		poderia ser feita de forma \textbf{aleatória}. 
		
		\item<4-> Porém, normalmente se utiliza uma política que inicialmente escolhe 
		aleatoriamente as ações, e, à medida que vai aprendendo, passa a utilizar cada 
		vez mais as decisões determinadas pela política derivada de $Q$. 
		
		\item<5-> Esta estratégia inicia \textbf{explorando} (tentar uma ação mesmo que ela não 
		tenha o maior valor de $Q$) e termina escolhendo a ação que tem o 
		maior valor de $Q$ (\textit{exploitation}).  
		
	\end{itemize}
\end{frame}


\begin{frame}{Exemplo de função para escolha de ações}
	
	A escolha de uma ação para um estado é dada pela função:
	
	\vspace{0.3cm}
	
	\begin{algorithmic} 
		\STATE \textbf{function} escolha($s$, $\epsilon$): $a$
		\STATE rv = random ($0 < rv \leq 1$)
		\IF{$rv < \epsilon$}
		\STATE \textbf{return} uma ação $\alpha$ aleatória em $A$
		\ENDIF   
		\STATE \textbf{return} $\max_{a}{Q(s, a)} $
	\end{algorithmic}

	\vspace{0.3cm}
	
	O fator de exploração $\epsilon$ ($0 \leq \epsilon \leq 1$) inicia com um valor 
	alto ($0.7$, por exemplo) e, conforme a simulação avança, 
	diminiu: $\epsilon \leftarrow \epsilon \times \epsilon_{dec}$, 
	onde $\epsilon_{dec} = 0.99$
	
\end{frame}


\begin{frame}{Epsilon}
	  \begin{center}
		\includegraphics[width=\textwidth]{figuras/epsilon.png}
	\end{center}
\end{frame}

\begin{frame}{Algoritmo Q-Learning}
	
\begin{algorithmic} 
	\STATE \emph{\textbf{function} Q-Learning(env, $\alpha$, $\gamma$, $\epsilon$, $\epsilon_{min}$, $\epsilon_{dec}$, episódios)}
	\STATE inicializar os valores de $Q(s, a)$ arbitrariamente
	\FOR {todos os episódios}
	\STATE inicializar $s$ a partir de $env$
	\REPEAT
	\STATE $a \leftarrow escolha(s, \epsilon)$
	\STATE $s', r \leftarrow$ executar a ação $a$ no $env$
	\STATE $Q(s,a) \leftarrow Q(s,a) + \alpha [r +\gamma \max_{a'}{Q(s', a')} - Q(s,a)]$
	\STATE$s  \leftarrow s'$
	\UNTIL {$s$ ser um estado final}
	\STATE \textbf{if} $\epsilon > \epsilon_{min}$ \textbf{then} $\epsilon \leftarrow \epsilon \times \epsilon_{dec}$
	\ENDFOR
	\STATE \textbf{return} Q
\end{algorithmic}	
\end{frame}

\begin{frame}{Atividade de implementação}
		
	\begin{alertblock}{Hiperparâmetros e seleção das ações}
		O objetivo desta atividade é compreender o funcionamento e impacto dos hiperparâmetros de $\alpha$, $\gamma$ e dos conceitos de \textit{exploration} e \textit{exploitation}.
	\end{alertblock}
	
	\begin{block}{Atividades}
		Siga o roteiro descrito em src/parte2/parte2.md \href{https://github.com/fbarth/reinLearn/blob/main/src/parte2/parte2.md}
		{\beamergotobutton{Link}}
	\end{block}

\end{frame}

\begin{frame}{Ambientes não-determinísticos}
	
	\begin{alertblock}{Ambientes não-determinísticos}
		O objetivo desta atividade é desenvolver agentes capazes de atuar em ambientes não-determinísticos.
	\end{alertblock}
	
	\begin{block}{Atividades}
		Siga o roteiro descrito em src/parte3/parte3.md \href{https://github.com/fbarth/reinLearn/blob/main/src/parte3/parte3.md}
		{\beamergotobutton{Link}}
	\end{block}
\end{frame}


\begin{frame}{Ambiente competitivo com variável aleatória}
	\begin{alertblock}{Jogos}
		O objetivo desta atividade é utilizar e analisar o comportamento de agentes em ambientes competitivos com variável aleatória.
	\end{alertblock}
	
	\begin{block}{Atividades}
		Siga o roteiro descrito em src/parte4/parte4.md \href{https://github.com/fbarth/reinLearn/blob/main/src/parte4/parte4.md}
		{\beamergotobutton{Link}}
	\end{block}
\end{frame}

\begin{frame}{Ambiente contínuo}
	
	\begin{alertblock}{Espaço de estados contínuo}
		O objetivo desta atividade é fazer uso de aprendizagem de reforço em ambientes com um espaço de estados contínuo.
	\end{alertblock}
	
	\begin{block}{Atividades}
		Siga o roteiro descrito em src/parte5/parte5.md \href{https://github.com/fbarth/reinLearn/blob/main/src/parte5/parte5.md}
		{\beamergotobutton{Link}}
	\end{block}
\end{frame}


%%\begin{frame}{Q-Learning com GridSearch}
%%Faz sentido usar GridSearch para encontrar os melhores valores de $\alpha$ e $\gamma$? 
%%\end{frame}



\begin{frame}{Considerações Finais}
\begin{itemize}
	\item O algoritmo \textit{Q-Learning} pode ser utilizado por agentes que 
	não tem conhecimento prévio sobre o problema.
	
	%\item Diversos autores já provaram que o algoritmo \textit{Q-Learning} converge 
	%para a função correta $Q$ dentro de certas condições. Por exemplo, uma 
	%delas é garantir que o agente avalie um par $Q(s,a)$ diversas vezes.  
	
	%\newpage
	
	\item \textit{Q-Learning} converge em ambientes 
	\textbf{determinísticos} e \textbf{não-determinísticos}.
	
	\item Na prática, o algoritmo \textit{Q-Learning} necessita de muitas iterações de 
	treinamento até convergir, inclusive para problemas que não tem um espaço de busca 
	tão grande.   

\pause 

	\item \textbf{E quando o espaço de busca for muito grande?} 

	%\item Usar \textit{Deep Learning} com \textit{Reinforcement Learning}! \emph{Assunto do 
	%nosso próximo encontro!}

	%\item Ler o capítulo \textit{18. Reinforcement Learning} até a seção 
	%\textit{Implementing Deep Q-Learning} do livro \textbf{Hands-On Machine Learning 
	%with Scikit-Learn, Keras, and TensorFlow, 2nd Edition} do Aurélien Géron.

	%\item Referência adicional: https://deepmind.com/research/case-studies/alphago-the-story-so-far

\end{itemize}
\end{frame}


\begin{frame}{O que vimos até o momento?}
\begin{itemize}
	\item o que é \textbf{Aprendizagem por Reforço} e os seus principais conceitos;
	\item como desenvolver um agente autônomo usando o algoritmo \textbf{Q-Learning};
	\item quais os aspectos positivos e negativos do algoritmo \textbf{Q-Learning}, e;
	\item funcionamento da biblioteca \textbf{Gym}. 
\end{itemize}
\end{frame}

\begin{frame}{Material de \textbf{consulta}}
\begin{itemize}
  \item Tom Mitchell. Machine Learning. McGraw-Hill, 1997.
  \item Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. Second Edition, in progress. The MIT Press, 2015.
  \item Projeto Gym: \href{https://gym.openai.com/}{https://gym.openai.com/} e \href{https://www.gymlibrary.ml/}{https://www.gymlibrary.ml/}
  %\item https://deepmind.com/research/case-studies/alphago-the-story-so-far
  %\item Aurélien Géron. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition, 2019. 
\end{itemize}
\end{frame}


\end{document}

